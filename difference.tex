%  LaTeX support: latex@mdpi.com 
%DIF LATEXDIFF DIFFERENCE FILE


%  In case you need support, please attach all files that are necessary for compiling as well as the log file, and specify the details of your LaTeX setup (which operating system and LaTeX version / tools you are using).

%=================================================================
\documentclass[journal,article,accept,moreauthors,pdftex, applsci]{Definitions/mdpi} 

% If you would like to post an early version of this manuscript as a preprint, you may use preprint as the journal and change 'submit' to 'accept'. The document class line would be, e.g., \documentclass[preprints,article,accept,moreauthors,pdftex]{mdpi}. This is especially recommended for submission to arXiv, where line numbers should be removed before posting. For preprints.org, the editorial staff will make this change immediately prior to posting.

%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g., the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.

%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.

%=================================================================
\firstpage{1} 
\makeatletter 
\setcounter{page}{\@firstpage} 
\makeatother
\pubvolume{xx}
\issuenum{1}
\articlenumber{5}
\pubyear{2019}
\copyrightyear{2019}
%\externaleditor{Academic Editor: name}
\history{Received: date; Accepted: date; Published: date}
%\updates{yes} % If there is an update available, un-comment this line

%% MDPI internal command: uncomment if new journal that already uses continuous page numbers 
%\continuouspages{yes}

%------------------------------------------------------------------
% The following line should be uncommented if the LaTeX file is uploaded to arXiv.org
%\pdfoutput=1

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, calc, indentfirst, fancyhdr, graphicx, lastpage, ifthen, lineno, float, amsmath, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, amsthm, hyphenat, natbib, hyperref, footmisc, geometry, caption, url, mdframed, tabto, soul, multirow, microtype, tikz

\usepackage[single=true, macros=true, xspace=true]{acro}  % For acronyms
\usepackage{cleveref} % For references
\usepackage[squaren,Gray]{SIunits}
\usepackage{subcaption}
%DIF 47d47
%DIF < % \usepackage{tikz}
%DIF -------


%=================================================================
%% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition, Notation, Assumption
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=====================================
% INPUTS
%=====================================
\input{acronyms.tex} 

%=================================================================
% Full title of the paper (Capitalized)
\Title{Classification of lentigo maligna at patient-level by means of reflectance confocal microscopy data}

%DIF 63a62-65
%===================================== %DIF > 
% MACRO %DIF > 
%===================================== %DIF > 
\newcommand{\fscore}{$F_{1}$ score} %DIF > 
%DIF -------
% Author Orchid ID: enter ID or remove command
\newcommand{\orcidauthorA}{0000-0002-6449-9723} % Cendre Add \orcidA{} behind the author's name
\newcommand{\orcidauthorB}{0000-0001-9054-3719} % Mansouri
\newcommand{\orcidauthorC}{0000-0003-4479-9961} % Perrot 
\newcommand{\orcidauthorD}{0000-0002-4009-0659} % Cinotti 
\newcommand{\orcidauthorE}{0000-0003-0963-1565} % Marzani 


% Authors, for the paper (add full first names)
\Author{Romain Cendre~$^{1,*}$\orcidA{}, Alamin Mansouri~$^{1}$\orcidB{}, Jean-Luc Perrot~$^{2}$\orcidC{}, Elisa Cinotti~$^{3}$\orcidD{} and Franck Marzani~$^{1}$\orcidE{}}

% Authors, for metadata in PDF
\AuthorNames{Romain Cendre, Alamin Mansouri, Jean-Luc Perrot, Elisa Cinotti and Franck Marzani}

\address{%
$^{1}$ \quad Laboratoire ImViA (EA 7535), Université Bourgogne Franche-Comté, Dijon, France; imvia.direction@u-bourgogne.fr\\
$^{2}$ \quad Service de Dermatologie-Oncologie-Allergologie, CHU de Saint-Etienne, Saint-Etienne, France\\
$^{3}$ \quad U.O. Dermatologia, Dipartimento di Scienze Mediche, Chirurgiche e Neuroscienze, A.O.U.S. Le Scotte - Università degli Studi di Siena, Siena, Italia}

% Contact information of the corresponding author
\corres{Correspondence: romain.cendre@gmail.com}

% Abstract
%DIF 86c89
%DIF < \abstract{Reflectance confocal microscopy is an appropriate tool for the diagnosis of lentigo maligna. Compared with dermoscopy, this device can provide abundant information as a mosaic and/or a stack of images. In this particular context, the number of images per patient varied between 2 and 833 images and the objective, ultimately, is to be able to discern between benign and malignant classes. First, this paper evaluated classification at the image level, with the help of handcrafted methods derived from the literature and transfer learning methods. Secondly, this work proposes patient-level supervised methods based on image decisions and a comparison of these with multi-instance learning methods. In terms of image-level classification, this study achieved an F1 score of 0.82 by the use of “Inception-ResNet” architecture and Support Vector Machine with a linear kernel. At the patient-level and based on “Inception-ResNet” architecture for features extraction, this study reached an F1 score of 0.83 (Sensitivity 0.88 / Specificity 0.75) with an \acs{auc} score of 0.87 for the dynamic threshold over decisions and achieve weighted F1-Score of 0.80 (Sensitivity 0.78 / Specificity 0.84) with an \acs{auc} score of 0.88 for multi-instance learning based on MI-SVM.}\par
%DIF -------
\abstract{Reflectance confocal microscopy is an appropriate tool for the diagnosis of lentigo maligna. Compared with dermoscopy, this device can provide abundant information as a mosaic and/or a stack of images. In this particular context, the number of images per patient varied between 2 and 833 images and the objective, ultimately, is to be able to discern between benign and malignant classes. First, this paper evaluated classification at the image level, with the help of handcrafted methods derived from the literature and transfer learning methods. Secondly, this work proposes patient-level supervised methods based on image decisions and a comparison of these with multi-instance learning methods. In terms of image-level classification, this study achieved an \fscore{} of 0.82 by the use of “Inception-ResNet” architecture and Support Vector Machine with a linear kernel. At the patient-level and based on “Inception-ResNet” architecture for features extraction, this study reached an \fscore{} of 0.83 (Sensitivity 0.88 / Specificity 0.75) with an \acs{auc} score of 0.87 for the dynamic threshold over decisions and achieve weighted \fscore{} of 0.80 (Sensitivity 0.78 / Specificity 0.84) with an \acs{auc} score of 0.88 for multi-instance learning based on MI-SVM.}\par %DIF > 
%DIF -------

% Keywords
\keyword{Computer-assisted diagnosis; Classification; Transfer Learning; Reflectance Confocal Microscopy; Dermatology; Lentigo}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Applied Sciences:
\featuredapplication{This paper focuses on improvement in patient care and it also helps practitioners optimize their dermatology services by means of computer-assisted diagnostic software using data from reflectance confocal microscopy devices.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%DIF PREAMBLE EXTENSION ADDED BY LATEXDIFF
%DIF UNDERLINE PREAMBLE %DIF PREAMBLE
\RequirePackage[normalem]{ulem} %DIF PREAMBLE
\RequirePackage{color}\definecolor{RED}{rgb}{1,0,0}\definecolor{BLUE}{rgb}{0,0,1} %DIF PREAMBLE
\providecommand{\DIFadd}[1]{{\protect\color{blue}\uwave{#1}}} %DIF PREAMBLE
\providecommand{\DIFdel}[1]{{\protect\color{red}\sout{#1}}}                      %DIF PREAMBLE
%DIF SAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddbegin}{} %DIF PREAMBLE
\providecommand{\DIFaddend}{} %DIF PREAMBLE
\providecommand{\DIFdelbegin}{} %DIF PREAMBLE
\providecommand{\DIFdelend}{} %DIF PREAMBLE
%DIF FLOATSAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddFL}[1]{\DIFadd{#1}} %DIF PREAMBLE
\providecommand{\DIFdelFL}[1]{\DIFdel{#1}} %DIF PREAMBLE
\providecommand{\DIFaddbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFaddendFL}{} %DIF PREAMBLE
\providecommand{\DIFdelbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFdelendFL}{} %DIF PREAMBLE
%DIF END PREAMBLE EXTENSION ADDED BY LATEXDIFF

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%=====================================
% INTRODUCTION
%=====================================
\section{Introduction}
As the incidence rate for skin cancers has steadily increased over the years, they are now the most prevalent form of human malignancy. These diseases affect people in their everyday life, as they have a pronounced social impact on the affected individuals as a result of a decrease in their quality of life as well as because they have the potential to become lethal. In addition, they have significant economic consequences, with an estimated cost of 8 billion dollars per year in the United States~\cite{Farberg2017a}, and they can stretch the ability of dermatological centers to cater to the at times overwhelming demand for screening and treatment. However, most of these repercussions can be avoided by early detection and appropriate surgeries~\cite{Farberg2017a}.\par
Currently, \DIFdelbegin \DIFdel{histological examination }\DIFdelend \DIFaddbegin \DIFadd{clinicopathological correlation }\DIFaddend is the gold standard \DIFdelbegin \DIFdel{for diagnosing skin cancersin the clinical context. This process }\DIFdelend \DIFaddbegin \DIFadd{to diagnose skin cancers. The histological examination }\DIFaddend is relatively time-consuming as it requires excision of the affected area, \DIFaddbegin \DIFadd{the }\DIFaddend embedding of the sample in paraffin, the generation of thin tissue sections that then need to be stained and examined by a histopathologist. \DIFdelbegin \DIFdel{However, despite }\DIFdelend \DIFaddbegin \DIFadd{Despite }\DIFaddend its accuracy, this technique remains time-consuming, invasive, and inconvenient for doctors and patients. Consequently, several non-invasive imaging techniques have been developed to help \DIFdelbegin \DIFdel{with the classification }\DIFdelend \DIFaddbegin \DIFadd{the clinical diagnosis }\DIFaddend of skin cancers, and some of these are now commonly used by dermatologists. For instance, clinical photography and dermoscopy are both examples of affordable and intuitive techniques that are presently widely used by dermatologists. Dermoscopy tends to replace clinical photography as it significantly improves the quality of the diagnoses made by experts, due largely to the acquisition of high-magnification images of the skin~\cite{Sinz2017}.\par
Research papers on dermatology nowadays tend to focus on the dermoscopy modality used to perform automatic classification of lesions. Most of them obtain acceptable results with melanocytic pathologies~\cite{Iyatomi2010}. Older methods focus on finding the most pertinent combination of preprocessing steps and handcrafted features to be used in a machine learning scheme~\cite{Rastgoo2015,Pathan2018}. By contrast, most recent methods use deep learning approaches, and they have yielded impressive results in this discipline~\cite{Esteva2017}. In this particular study, the authors used an Inception-V3 architecture pre-trained on the “ImageNet” database~\cite{Deng2008}, and they fine-tuned this model on a dataset of 129,450 clinical images containing 2,032 different skin lesions and distributed across 757 classes. They carried out the classification at different taxonomy levels, and at the first level of classification (Non-neoplastic versus \DIFdelbegin \DIFdel{Benign versus Malignant}\DIFdelend \DIFaddbegin \DIFadd{benign versus malignant}\DIFaddend ), they achieved an accuracy of 0.72$\pm$0.9\ compared to 0.66 on a subset of these data by specialists.\par
However, dermoscopy imaging devices only provide surface and chromatic information. To overcome this limitation, \ac{rcm} modality is another type of imaging technique used by dermatologists that provides high-resolution images of the skin on a micrometer scale. Furthermore, this modality can provide structural information at different depths of the skin by adjustment of the wavelength properties and the focal point~\cite{Kolm2012}. The \ac{rcm} device was first designed by Marvin Minksy. The principle is to emit and focus a low power laser on a specific point of the skin (see \Cref{fig:rcm} - Emission). Then the light is reflected from this point into the system optic and collected through a pinhole that allows only the light from \DIFdelbegin \DIFdel{in focus }\DIFdelend \DIFaddbegin \DIFadd{the in-focus }\DIFaddend plane (see \Cref{fig:rcm} - Reception). In this situation, the illuminated point and the detector aperture have confocal (a contraction of conjugate focal) planes~\cite{Nehal2008a}. Different factors can affect the depth: illumination wavelength, illumination power, reflective and scattering properties of the skin.\par

\begin{figure}[H]
    \begin{center}
        \DIFdelbeginFL %DIFDELCMD < \includegraphics[width=0.7\linewidth]{Figures/RCM.pdf}
%DIFDELCMD <         %%%
\DIFdelendFL \DIFaddbeginFL \includegraphics[width=0.6\linewidth]{Figures/RCM.pdf}
        \DIFaddendFL \caption{Scheme of the \ac{rcm} principle in two steps: a low power laser is first emitted (Emission), then the reflected light is received through a pinhole (Reception).}
        \label{fig:rcm}
    \end{center} 
\end{figure}\par

Presently, this tool has \DIFdelbegin \DIFdel{a }\DIFdelend greater diagnostic accuracy compared to dermoscopy, both for melanocytic and for non-melanocytic skin tumors~\cite{Haroon2017, Dinnes2018, Lupu2019}. Unlike the previous modalities, \ac{rcm} remains expensive, although the number of users continues to increase~\cite{Batta2015} and recent developments have led to a degree of improvement in the portability of \ac{rcm} devices~\cite{Freeman2018}.\par
By contrast, relatively fewer studies have been published on the \ac{rcm} modality by use of computer vision techniques, despite their promising results in the clinical context. The use of artificial intelligence could be particularly useful for \ac{rcm} images because their evaluation by dermatologists is time-consuming. Indeed, unlike with dermoscopy, many \ac{rcm} images need to be acquired for each skin lesion. Many studies of computer vision focus on understanding these images by predicting the position in the skin layer \cite{Somoza2014,Hames2016} using stacks of 3D data provided by this modality. A number of other studies have described the structural components of the skin~\cite{Gareau2010} and few of them have classified pathologies based on that specific modality. One of these studies~\cite{Wiltgen2008} is quite similar to the problem explored in this work, and it suggests several image descriptors for undertaking a classification task. These authors introduced two methods using frequency representation: the first one based on Fourier transform and the second one based on wavelet representation by use of Daubechies 4. The idea behind spectral representation is to extract information at different frequency levels, thereby yielding information about smoothness or complex structures inside the images, and to be consistent with rotations and translations as the pathologies are non-oriented. The second category of descriptors that they employed was based on spatial features by use of \ac{glh} and \ac{glcm} statistical descriptors. The statistical descriptors computed from \ac{glcm} had been derived from \DIFdelbegin \DIFdel{a }\DIFdelend previous work~\cite{Haralick1973}. Finally, the authors proceeded with the classification of images by using different image parts at multiple sizes and they proceeded to \ac{cart} classification. In a two-class situation, this paper reached an accuracy of 0.96 for the detection of nevi and 0.97 for melanoma pathologies by applying a wavelet extraction and \ac{cart} classification on a subpart of the 256*256 pixel images. Another paper is more relevant, as the authors followed the same purpose as this work by suggesting a way to classify solar lentigo pathologies by use of the previous wavelet decomposition, and by fitting the decomposition values to a \ac{ggd} to reduce the number of variables. They also suggested that only one variable and only one scale decomposition are relevant for solar lentigo detection. This method was applied to 45 subjects with healthy skin or solar lentigo, and it achieved a sensitivity of 0.81 and a specificity of 0.83~\cite{Halimi2017a}.\par
The scope of this work is to detect malignant tumors and particularly \ac{lm} (the most common type of facial melanoma) in \ac{rcm} images \DIFdelbegin \DIFdel{, }\DIFdelend and to help specialists reach a diagnosis based on these images at the patient-level. The previous feature extraction methods and extraction through different \DIFdelbegin \DIFdel{convolutional neural network }\DIFdelend \ac{cnn} architectures were investigated first. As wavelet decomposition and reduction through \ac{ggd}~\cite{Halimi2017a} were shown to be irrelevant in our data context~\cite{Cendre2019a}, this work did not focus on any of these methods. \DIFdelbegin \DIFdel{Comparison }\DIFdelend \DIFaddbegin \DIFadd{A comparison }\DIFaddend of several classification models on full-size images was then carried out to estimate the relevance of these methods. \DIFaddbegin \DIFadd{Also, this study will employ the term “lentigo maligna” instead of the term “melanoma in situ type lentigo maligna” as it was used by the last }\ac{rcm} \DIFadd{clinical studies \mbox{%DIFAUXCMD
\cite{Mataca2018, NAVARRETEDECHENT2019, Robinson2019, Hao2019} }\hspace{0pt}%DIFAUXCMD
and for the sake of simplicity.}\DIFaddend \par
The following parts of this paper are organized as follows. \Cref{sec:material} covers the data by proving details about their composition, the feature extraction methods implemented, and the process used to compute image-level and patient-level decisions. \Cref{sec:results} then displays all of the results and provides an analysis of them, and finally, \Cref{sec:conclusions} provides a conclusion of this work and it offers a number of perspectives.\par

%=====================================
% Materials
%=====================================
\section{Materials and Methods}
\label{sec:material}

%=====================================
% DATA
%=====================================
\subsection{Data}
\label{sec:data}
The data for this paper were originally obtained in a previous clinical study~\cite{Cinotti2018} that performed a comparison between dermoscopy and \ac{rcm} modalities in the diagnosis of benign lesions such as solar lentigo and malignant tumors by focusing essentially on \ac{lm}. These images were acquired by three specialists, experts in regard to non-invasive skin imaging tools, by use of a hand-held VivaScope® 3000 camera that uses a laser with a wavelength of \unit{830}{\nano\meter} and images to a depth of up to \unit{250}{\micro\meter}. In addition, these data included lesions that can be highly misleading for dermatology specialists. \DIFaddbegin \DIFadd{From an ethical perspective, this study was conducted following the Declaration of Helsinki and the protocol was approved by the Ethics Committee of the University Hospital of Saint-Etienne (Institutional review board number 672016/CHUSTE).}\DIFaddend \par
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend Only images considered relevant for the diagnosis by two out of the three investigators were retained, at different depths of the skin: the epidermis, the \ac{dej}, and the dermis. However, most of the information was acquired at the \ac{dej}. Each \ac{rcm} image \DIFdelbegin \DIFdel{corresponded }\DIFdelend \DIFaddbegin \DIFadd{corresponding }\DIFaddend to a horizontal \unit{920}{\micro\meter} x \unit{920}{\micro\meter} section of the skin at a selected depth with a lateral resolution of \unit{1}{\micro\meter} and axial resolution of \unit{3}{\micro\meter} to \unit{5}{\micro\meter}. For specifications, these images have a spatial resolution of 1000*1000 pixels with quantification on a single 8-bits channel.\par
The relative position between images of a single patient was unknown, and this could not provide any further knowledge in the next part of this work. Furthermore, the metadata available regarding the age of the patients was not used as the purpose of this work was to evaluate the relevance of image classification techniques, although it was initially provided to the experts during their assessments.\par
These data included 223 patients, for a total of 7,846 RCM images, varying between 2 and 833 images per patient (with a mean of 35 and a standard deviation of 64). For each patient, the data provided a \DIFdelbegin \DIFdel{histopathology diagnosis }\DIFdelend \DIFaddbegin \DIFadd{diagnosis based on clinicopathological correlation }\DIFaddend that served as a reference basis and there was the following distribution of cases:
\begin{itemize}
\item 135 patients had “malignant” tumors: 115 \ac{lm} and 20 \ac{bcc}.
\item 88 patients had “benign” tumors: mainly represented by solar lentigines.
\end{itemize}\par
\begin{figure}[H]
    \begin{center}
        \includegraphics[width=\linewidth]{Figures/Data.pdf}
        \caption{This figure shows several examples of images related to the three different types of tissue: namely “healthy”, “benign” and “malignant”. For this work purpose, the “healthy” and “benign” labels are merged under the “benign” label.}
        \label{fig:data}
    \end{center} 
\end{figure}\par
\newpage
\DIFaddbegin \DIFadd{No collision tumors were included in this series. }\DIFaddend Also, the study~\cite{Cinotti2018} evaluated 21 experts, and they achieved a mean sensitivity of 0.80 \DIFaddbegin \DIFadd{(range 0.66-0.90, standard deviation 0.07) }\DIFaddend and a specificity of 0.81 \DIFdelbegin \DIFdel{, }\DIFdelend \DIFaddbegin \DIFadd{(range 0.73-0.90, standard deviation 0.05), }\DIFaddend with an \ac{auc} score of 0.89 for the detection of \ac{lm}.\par
In order to reduce the imbalance of the data, a further 28 additional benign tumors were provided by dermatologists, with the number of images per tumor varying between 4 and 103 images (608 images in total). These new patients were only considered for training purposes and they were not taken into account in \Cref{sec:results}.\par
In addition, these data did not provide any information regarding individual images. As these annotations were required in the next part of the work, each of these images was annotated by a specialist, with the help of a graphical interface designed for this purpose. Images with \ac{bcc} tumors were not considered as the objective is to be able to discern \ac{lm}. Whereas the patient labels were “benign” or “malignant”, some of the images could not be classified into either of these two categories because they did not contain any of these pathology signs. For this particular reason, a “healthy” label was introduced to characterize them. \Cref{fig:data} provides an overview of these different data. As this study focused only on binary classification of malignant diseases, an annotation hierarchy was defined as follows:
\begin{itemize}
\item A “malignant” label: an image with at least some malignant tissues from \ac{lm} tumors.
\item A “benign” label: an image with no malignant tissues (either “benign” or “healthy” skin).
\end{itemize}
These annotated images amounted to 5,277 images, divided equally between men and women. The “malignant” labels accounted for 44\% of the annotated images, while the “benign” label accounted for 56\%.\par

%=====================================
% Features
%=====================================
\subsection{Feature extraction methods}
\label{sec:features}
In order to classify the data, a reduction of the image information had to be performed in a new feature space able to distinguish “malignant” from “benign” image types. According to the dermatologists, texture plays an important role in the differentiation of \DIFdelbegin \DIFdel{the }\DIFdelend tissue types. The first part of this section focuses on handcrafted feature extractors based on texture from \DIFdelbegin \DIFdel{a }\DIFdelend previous work~\cite{Wiltgen2008}. The deep extraction methods applied to this context and inspired by a previous work on dermoscopy images~\cite{Esteva2017} are then detailed. All of the feature extraction methods are listed in \Cref{tab:features_methods}, and the next parts follow this table structure.\par
\begin{table}[H]
    \centering
    \begin{tabular}{lll}
    \hline
    \textbf{Category}                   &  \textbf{Name}                & \textbf{Number of features}  \\ \hline
    \multirow{2}{*}{Spatial}            &  Haralick                     & 12                        \\ \cline{2-3} 
                                        &  \ac{glh}+\ac{glcm}           & 17                        \\ \hline 
    \multirow{2}{*}{Frequency}          &  Fourier                      & 38                        \\ \cline{2-3} 
                                        &  Wavelet                      & 39                        \\ \hline
    \multirow{4}{*}{Transfer Learning}  &  VGG-16                       & 512                       \\ \cline{2-3} 
                                        &  Inception-V3                 & 2048                      \\ \cline{2-3} 
                                        &  ResNet                       & 2048                      \\ \cline{2-3} 
                                        &  Inception-ResNet             & 1536                      \\ \hline
    \end{tabular}
    \caption{The list of all of the feature extraction methods performed in this paper and their associated extracted number of features.}
    \label{tab:features_methods}
\end{table}\par
The “Spatial” extraction methods were based on spatial patterns of pixels, by use of \ac{glh} and \ac{glcm}. The method called “Haralick” refers to previous work based on texture features~\cite{Haralick1973} and it uses the \ac{glcm} concept by computation of the twelve statistical characteristics listed in \Cref{tab:histogram_features} - \ac{glcm} Features column. These characteristics were extracted along horizontal, vertical, and two diagonals. A mean was computed along these axes as the tissues are not oriented in space, and to reduce the number of features. A second method from \DIFdelbegin \DIFdel{a }\DIFdelend previous work~\cite{Wiltgen2008}, called “\ac{glh}+\ac{glcm}” in this paper, expanded the first twelve initial characteristics of Haralick and it added five others based on \ac{glh}. In total, 17 features were extracted for each image, and all of the statistical properties extracted are listed in \Cref{tab:histogram_features}. The Haralick features extraction was performed using the “Mahotas” library~\cite{coelho2012mahotas} and histogram feature extraction was computed with help from the “Scipy” library~\cite{Jones2001}.\par
\begin{table}[H]
    \centering
    \begin{tabular}{ll}
        \hline
        \textbf{\ac{glcm} Features}& \textbf{\ac{glh} Features}     \\ \hline
        Angular Second Moment      & Mean value                     \\
        Difference Moment          & Mean square deviation          \\
        Correlation                & Skewness                       \\
        Sum of Squares             & Kurtosis                       \\
        Inverse Difference Moment  & Entropy                        \\     
        Summed Average             &                                \\    
        Sum Variance               &                                \\    
        Entropy                    &                                \\    
        Sum Entropy                &                                \\    
        Difference Entropy         &                                \\    
        Measure of Correlation 1   &                                \\  
        Measure of Correlation 2   &                                \\ 
    \end{tabular}
    \caption{The statistical measures derived from \ac{glcm} and \ac{glh}, respectively, and extracted in order to perform “Spatial” extraction methods.}
    \label{tab:histogram_features}
\end{table}\par
\DIFdelbegin \DIFdel{A }\DIFdelend \DIFaddbegin \DIFadd{The }\DIFaddend second category of extraction methods, called “Frequency”, refers to a set of methods based on frequency approaches. The first method of this category is called “Fourier” and is based on \DIFaddbegin \DIFadd{the }\DIFaddend Fourier transform. The main idea is to provide different levels of information as high frequency refers to high-contrast parts and low frequencies to homogeneous areas in the image. As the spectrum is symmetrical around the origin, only half of this spectrum was considered for the sake of computational efficiency. Then, a mean value was computed for all of the coefficients located at the same radial distance from the origin, at 22 different radius sizes between 0 and the diagonal size of the image. A previous paper~\cite{Smach2008a} has also shown the relevance of this method in the context of textural images. Finally, 16 constant directions were taken from the origin of the power spectrum, and a mean value was computed for each of them~\cite{Wiltgen2008}. The second method of this category is called “Wavelet” in this work and is based on Wavelet transform by use of a decomposition based on a Daubechies 4 that provides quite fine localization properties~\cite{Wiltgen2008}. This decomposition was made at five scales, and only the four last scales were considered to compute coefficients. For each of them, three statistical measures were computed: the standard deviation, the energy, and the entropy.\par
%DIF <  \begin{figure}[h]
%DIF <      \begin{center}
%DIF <          \begin{tikzpicture}[scale=1.2]
%DIF <              \node[] at (-2, 0) {a)};
%DIF <              \begin{scope}
%DIF <                  \clip (-1.5, -1.5) rectangle (1.5, 1.5);
%DIF <                  \foreach \cRadius in {1, ..., 22}
%DIF <                      \pgfmathsetmacro\cColor{\cRadius/22*100} %22-22}
%DIF <                      \draw[red!\cColor!blue, densely dotted] (0, 0) circle (0.02+\cRadius*0.1); 
                    \DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
%DIF <                  \fill[blue!20] (-1.5, -1.5) rectangle (0, 1.5);
%DIF <                  \draw[blue!60, ultra thick] (-1.5, -1.5) rectangle (1.5, 1.5);
%DIF <              \end{scope}
%DIF <          \end{tikzpicture}
%DIF <          \qquad
%DIF <          \begin{tikzpicture}[scale=1.2]
%DIF <              \begin{scope}
%DIF <                  \clip (-1.5, -1.5) rectangle (1.5, 1.5);
%DIF <                  \foreach \cAngle in {1, ..., 16}
%DIF <                      \pgfmathsetmacro\cColor{\cAngle/16*100}
%DIF <                      \draw[red!\cColor!blue, densely dotted] (0, 0) -- (\cAngle* 180 / 15 -102:3); 
%DIF <                  \fill[blue!20] (-1.5, -1.5) rectangle (0, 1.5);
%DIF <                  \draw[blue!60, ultra thick] (-1.5, -1.5) rectangle (1.5, 1.5);
%DIF <              \end{scope}
%DIF <          \end{tikzpicture}
%DIF <          \qquad
%DIF <          \vrule
%DIF <          \qquad
%DIF <          \begin{tikzpicture}[scale=1.2]
%DIF <              \node[] at (-2, 0) {b)};
%DIF <              % First level
%DIF <              \draw[blue!60, thick] (-1.5, -1.5) rectangle (0, 0);
%DIF <              \draw[blue!60, thick] (0, -1.5) rectangle (1.5, 0);
%DIF <              \draw[blue!60, thick] (0, 0) rectangle (1.5, 1.5);
%DIF <              % Second level
%DIF <              \filldraw[blue!60, fill=blue!5, thick] (-1.5, 0) rectangle (-0.75, 0.75) node[pos=.5] {15};
%DIF <              \filldraw[blue!60, fill=blue!5, thick] (-0.75, 0) rectangle (0, 0.75) node[pos=.5] {14};
%DIF <              \filldraw[blue!60, fill=blue!5, thick] (-0.75, 0.75) rectangle (0, 1.5) node[pos=.5] {13};
%DIF <              % Third level
%DIF <              \filldraw[blue!60, fill=blue!15, thick] (-1.5, 0.75) rectangle (-1.5+0.375, 0.75+0.375) node[pos=.5] {12};
%DIF <              \filldraw[blue!60, fill=blue!15, thick] (-1.5+0.375, 0.75) rectangle (-1.5+0.75, 0.75+0.375) node[pos=.5] {11};
%DIF <              \filldraw[blue!60, fill=blue!15, thick] (-1.5+0.375, 0.75+0.375) rectangle (-1.5+0.75, 0.75+0.75) node[pos=.5] {10};
%DIF <              \filldraw[blue!60, fill=blue!35, thick] (-1.5, 0.75+0.375) rectangle (-1.5+0.375, 0.75+0.75) node[pos=.5] {...};
%DIF <          \end{tikzpicture}
%DIF <      \end{center}
%DIF <      \caption{On the left, the feature extraction process by Fourier transform, first by drawing circular masks (22 in total) and then taking directions (16 in total). On the right, the wavelet decomposition described with an applied measure of 0-16.}
%DIF <      \label{fig:handcrafted}
%DIF <  \end{figure}\par
\DIFdelend The third category of methods investigated was in regard to deep learning methods and more specifically in regard to \ac{cnn}, which are known to be well-suited methods for image classification, thanks to robust feature patterns~\cite{Pathan2018}. Many architectures were used to address ImageNet challenges, and their associated performances were analyzed~\cite{Canziani2016}. Instead of training this network from scratch, as we have data constraints and also computational constraints, we choose a Domain Adaptation approach for these models. Most of the papers to date have dealt with \ac{cnn} trained on ImageNet~\cite{Deng2008}, as this database contains thousands of classes and more than 14 million images, meaning that the extracted features from these networks can be used in various fields. As discussed in a previous work~\cite{Litjens2017}, Inception-V3 architecture pre-trained on ImageNet is thought to be the most relevant for medical applications. As \ac{rcm} images can have various forms and can contain specifics details compared to other image modalities, this research compares the most well known \ac{cnn} architectures: VGG-16~\cite{Simonyan2014}, Inception-V3~\cite{Szegedy2015}, ResNet~\cite{He2016} and Inception-ResNet~\cite{Szegedy2017}, with accuracies of 0.71, 0.76, 0.78, and 0.80, respectively, on the ImageNet database~\cite{Canziani2016}. This method involves \DIFaddbegin \DIFadd{the }\DIFaddend use of Transfer Learning, by removing the last layers devoted to the classification task, in order to obtain a new representation of the image data as features. Furthermore, in order to reduce the number of features provided by the previous step, a global pooling layer on each activation layer was performed. For convenience, this whole method is called “Transfer Learning” in the next paragraphs and the names of the respective networks are used. The \ac{cnn} computation was implemented using the “Keras” library~\cite{chollet2015keras}.\par

%=====================================
% IMAGE LEVEL
%=====================================
\subsection{Image-level decision}
\label{sec:image_decision}
The image-level decision was the first level of classification achieved in this study, whereby the image classification must be carried out according to the two classes “malignant”, as the positive class, and “benign”. In order to satisfy this objective, the process consisted of using the image as a single instance that has several discriminant characteristics and sufficient information to allow its classification. As formulated by~\cite{foulds_frank_2010}, such a problem can be set as a pair {X|y}, in which \(X=\{x_1,x_2,\ldots,x_n\}\) is a vector characteristic for which \(n\) is the number of features and \(y\) the associated label. The task consisted of finding the existing relationship between \(X\) and \(y\), using a classification process.\par 
\begin{figure}[H]
    \begin{center}
        \includegraphics[width=0.7\linewidth]{Figures/Process_Image.pdf}
        \caption{The classification process performed on the \ac{rcm} images. The “Extraction” box refers to one of the Feature Extraction methods mentioned in \Cref{sec:features}. The “Fit Model” and “Prediction” boxes are related to the training and inference steps, respectively, of one of the trained and inferred models discussed in \Cref{sec:image_decision}. The testing set was predicted based on two classes: “benign” and “malignant”}
        \label{fig:image_process}
    \end{center} 
\end{figure}\par
To achieve this task, an extraction method (see \Cref{sec:features}) was applied to the images, depending on the currently evaluated method. The features were then normalized based on a standard score computation to make the classification task more accurate and robust~\cite{Graf2001}. This scaling was computed by subtracting the mean and then dividing by the standard deviation. The schematic outline in \Cref{fig:image_process} provides an overview of this process.\par
\begin{table}[H]
    \centering
    \begin{tabular}{lll}
    \textbf{Name}                                   & \textbf{Parameter}& \textbf{Values}                           \\ \hline
    \DIFdelbeginFL %DIFDELCMD < \multirow{2}{*}{\ac{cart}}      %%%
\DIFdelendFL \DIFaddbeginFL \multirow{2}{*}{\ac{cart}/\acs{rf}/\acs{ert}}   \DIFaddendFL & Maximum depth     & [3, $\infty$]                             \\ \cline{2-3}
    \DIFaddbeginFL \acreset{rf}\acreset{ert}                       \DIFaddendFL & Criterion         & [Gini, Entropy]                           \\ \hline 
    \DIFdelbeginFL %DIFDELCMD < \multirow{2}{*}{\acs{ert}}      %%%
\DIFdelendFL \DIFaddbeginFL \multirow{2}{*}{\acs{gb}}                       \DIFaddendFL & Maximum depth     & [3, $\infty$]                             \\ \cline{2-3}
    \DIFdelbeginFL %DIFDELCMD < \acreset{ert}                   %%%
\DIFdelendFL \DIFaddbeginFL \acreset{gb}                                    \DIFaddendFL & Criterion         & [\DIFdelbeginFL \DIFdelFL{Gini, Entropy}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{Mean squared error, Mean absolute error}\DIFaddendFL ] \\ \hline 
    \acs{svm} - Linear                              & C                 & [0.01, 0.1, 1, 10, 100, 1000]             \\ \hline
    \multirow{2}{*}{\acs{svm} - RBF}                & C                 & [0.01, 0.1, 1, 10, 100, 1000]             \\ \cline{2-3}
    \acreset{svm}                                   & Gamma             & [0.01, 0.1, 1, 10, 100, 1000]             \\ \hline 
    \end{tabular}    
    \caption{List of all of the classification models performed in this study and their referring evaluated hyper-parameters.}
    \label{tab:image_hyperparameters}
\end{table}\par
Finally, the classification was performed on scaled features using different models. In \DIFdelbegin \DIFdel{a }\DIFdelend \DIFaddbegin \DIFadd{the }\DIFaddend first stage, \ac{cart} \DIFdelbegin \DIFdel{were }\DIFdelend \DIFaddbegin \DIFadd{was }\DIFaddend investigated as in a previous study in the same data context~\cite{Wiltgen2008}. In a second stage, \DIFdelbegin \DIFdel{as our number of features can be huge due to Transfer Learning methods , }%DIFDELCMD < \ac{ert} %%%
\DIFdel{models were also used as an extension of }\DIFdelend \DIFaddbegin \DIFadd{this study explored alternatives of simple trees based on ensemble methods (set of models instead of a single model) as the }\DIFaddend \ac{cart} \DIFdelbegin \DIFdel{~\mbox{%DIFAUXCMD
\cite{Geurts2006}}\hspace{0pt}%DIFAUXCMD
. }\DIFdelend \DIFaddbegin \DIFadd{model tends to overfit. On the one hand, bagging methods were investigated by the use of }\ac{rf} \DIFadd{model~\mbox{%DIFAUXCMD
\cite{Breiman2001} }\hspace{0pt}%DIFAUXCMD
and }\ac{ert}\DIFadd{~\mbox{%DIFAUXCMD
\cite{Geurts2006} }\hspace{0pt}%DIFAUXCMD
as they both tend to remove the overfitting issues. Moreover, the }\ac{ert} \DIFadd{model are assumed to be more robust to noise than the }\ac{rf} \DIFadd{model. On the other hand, the boosting method was considered by the use of the }\ac{gb} \DIFadd{model~\mbox{%DIFAUXCMD
\cite{Friedman2000} }\hspace{0pt}%DIFAUXCMD
as the most common type of tree-based algorithm for most of the recent applications. }\DIFaddend Lastly, \ac{svm} models were evaluated that are known to be suitable in multiples contexts~\cite{Smach2008a,Kose2016b}. As the relationship between the features and the expected outputs can be complex, \ac{svm} models were compared over linear and RBF kernels.\par
In addition, to provide the best performance with each of these models, a search in regard to their optimum hyperparameters was carried out (see \Cref{tab:image_hyperparameters}).\par

%=====================================
% PATIENT LEVEL
%=====================================
\subsection{Patient-level decision}
\label{sec:patient_decision}
This part relates to different ways to achieve classification at the patient-level based on the same two categories: “malignant”, as the positive class, and “benign”. With this assumption, a patient should be considered “malignant” if at least one image is considered to be “malignant”. Additionally, this part needs to consider the varying number of samples per patient (as a reminder, the number of instances per patient can vary between 2 and 833 images).\par
In order to achieve this, the best combination of the feature extraction method and the classification model from \Cref{sec:image_decision} was used. The classification model provided two types of information for each image: the score was based on prediction probabilities and the decision (i.e., the class that achieved the best probability). In both cases, due to the varying number of images per patient, the information needs to be transformed into constant-size matrices to make a decision for existing and for new patients. At the score level, the structure was composed of patients, images, and scores of classes and transformed into a new matrix of size P*C, where P is the number of patients and C is the number of classes. A dynamic threshold was then used to adjust the positive class that maximizes the chosen metric. Multiple strategies can be used to achieve this:
\begin{itemize}
\item Mean - allows the contribution of each instance on the patient to be retained
\item Maximum – retention of the best confidence prediction as \DIFaddbegin \DIFadd{to }\DIFaddend the trusted one.
\end{itemize}
\begin{figure}[H]
    \begin{center}
        \includegraphics[width=0.8\linewidth]{Figures/Process_Decision.pdf}
        \caption{The classification process performed on \ac{rcm} patients in which the first part of the process remained the same as in \Cref{sec:image_decision}. The “Fit Model” and “Prediction” boxes refer to the decision and score level methods discussed in \Cref{sec:patient_decision}. The testing set was predicted for the patients based on “benign” and “malignant” classes.}
        \label{fig:decision_process}
    \end{center} 
\end{figure}\par
At the decision level, the structure was composed \DIFdelbegin \DIFdel{by }\DIFdelend \DIFaddbegin \DIFadd{of }\DIFaddend structure, combining patients, images, and scores of classes and transformed into a new matrix of size P*C, where P is the number of patients and C is the number of classes. Also, C refers to the probability vector of each decision between 0 and 1.
\begin{itemize}
\item At Least One - at least one positive decision to consider the input as positive (initial assumption)
\item Dynamic - find a dynamic threshold that minimizes false-positive decisions.
\end{itemize}
A global overview of the processing scheme of this method is presented in \Cref{fig:decision_process}.
In a second stage, a number of \ac{mil} concepts are implemented in this part, as they fit the issue at hand: a patient is constitutive of several instances (consider it as a bag) and a positive instance assumes that the patient should be positive. Furthermore, only the patient label is known, and the annotation step of individual images is time-consuming. Such a problem can be set as a pair\(\{X|y\}\), in which \(X=\{X^1,X^2,\ldots,X^b\}\) is a bag containing \(b\) instances and each \(X^b\) formulated as follows: \(X^b=\{x^b_1,x^b_2,\ldots,x^b_n\}\), in which \(n\) is the number of features and \(y\) is the patient label~\cite{foulds_frank_2010}. Two ideas are developed regarding this context in the paragraphs that follow. In a first stage, a \ac{sil} classification is used in which a bag is considered as negative if all of the instances are considered to be negative, and positive if at least one of the instances has a positive label that fits our initial formulation of the patient label. In a second stage, the MI-SVM \DIFdelbegin \DIFdel{are }\DIFdelend \DIFaddbegin \DIFadd{is }\DIFaddend an extension of \ac{svm} upon \ac{mil} theory and \DIFdelbegin \DIFdel{are }\DIFdelend \DIFaddbegin \DIFadd{is }\DIFaddend employed in these due to the results of \Cref{sec:image_decision}. These experiments are configured to function with a linear kernel due to the observation made by the experiments of \Cref{sec:image_decision}. In this part, the experiments were implemented using the “MISVM” library~\cite{Doran2014}.\par
In addition, to provide the best performance on each of these models, a search for their optimal hyper-parameters was carried out (see \Cref{tab:patient_hyperparameters}).\par
\begin{table}[H]
    \centering
    \begin{tabular}{llll}
    \textbf{Category}               & \textbf{Name}     & \textbf{Parameter}& \textbf{Values}                                   \\ \hline
    \multirow{2}{*}{Score}          & Mean              & \multirow{2}{*}{-}& \multirow{2}{*}{[]}                               \\ \cline{2-2}
                                    & Maximum           &                   &                                                   \\ \hline 
    \multirow{2}{*}{Decision}       & At Least One      & \multirow{2}{*}{-}& \multirow{2}{*}{[]}                               \\ \cline{2-2}
                                    & Dynamic           &                   &                                                   \\ \hline 
    \multirow{2}{*}{\ac{mil}}       & \ac{sil}          & \multirow{2}{*}{C}& \multirow{2}{*}{[0.01, 0.1, 1, 10, 100, 1000]}    \\ \cline{2-2}
                                    & MI-SVM            &                   &                                                   \\ \hline 
    \end{tabular}    
    \caption{List of all of the classification models and referring evaluated hyper-parameters.}
    \label{tab:patient_hyperparameters}
\end{table}\par

%=====================================
% RESULTS
%=====================================
\section{Results}
\label{sec:results}

\subsection{Validation and evaluation metric}
The validation protocol remains the same for each of these experiments, based on a nested cross-validation that is known to be less biased than a simple cross-validation scheme~\cite{Cawley2010}. This protocol allows 1) cross-validation of hyper-parameters and 2) objective evaluation of the prediction models. Each of the cross-validation \DIFdelbegin \DIFdel{steps }\DIFdelend \DIFaddbegin \DIFadd{step }\DIFaddend is based on a K-fold strategy with a $k$ value of 4 on the testing loop and 2 on the validation loop. Also, each time, the patients are separated and balanced as best as possible based on the image labels. In order to achieve an objective evaluation, each data cluster remains the same for the experiments in a given section (refer to \DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cref{sec:image_decision} }\hspace{0pt}%DIFAUXCMD
and \mbox{%DIFAUXCMD
\cref{sec:patient_decision}}\hspace{0pt}%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\Cref{sec:image_decision} }\hspace{0pt}%DIFAUXCMD
and \mbox{%DIFAUXCMD
\Cref{sec:patient_decision}}\hspace{0pt}%DIFAUXCMD
}\DIFaddend ). Moreover, each experiment is validated and evaluated using \DIFdelbegin \DIFdel{an F1 score }\DIFdelend \DIFaddbegin \DIFadd{a \fscore{} }\DIFaddend metric, as it is statistically suitable for unbalanced populations \DIFdelbegin \DIFdel{compared to an accuracymetric}\DIFdelend \DIFaddbegin \DIFadd{in comparison with accuracy}\DIFaddend , and it represents in a single value \DIFaddbegin \DIFadd{both }\DIFaddend recall and precision information. In addition, \DIFdelbegin \DIFdel{a }\DIFdelend standard deviation is computed to \DIFdelbegin \DIFdel{achieve an analysis of }\DIFdelend \DIFaddbegin \DIFadd{analyse }\DIFaddend the stability of models along the nested cross-validation. For this purpose, we used the “Scikit Learn” library for Machine Learning classification, validation, and metric~\cite{pedregosa2011scikit}.\par
\subsection{Experiments and Discussion}
The results of \Cref{sec:image_decision} regarding image-level decisions were performed only on the labeled images and are listed in \DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cref{tab:image_results}}\hspace{0pt}%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\Cref{tab:image_results}}\hspace{0pt}%DIFAUXCMD
}\DIFaddend . The most suitable handcrafted extraction method was based on the “Wavelet” method combined with the “\ac{svm} - Linear” model, \DIFdelbegin \DIFdel{and it reached an F1 score }\DIFdelend \DIFaddbegin \DIFadd{reaching an weighted \fscore{} }\DIFaddend of 0.74 with \DIFdelbegin \DIFdel{a quite }\DIFdelend stable performance of 0.04. In general, all of the handcrafted methods performed in a quite similar and stable way, varying from 0.69 to 0.74 for the \DIFdelbegin \DIFdel{F1 score }\DIFdelend \DIFaddbegin \DIFadd{\fscore{} }\DIFaddend and 0.03 to 0.07 for the deviation. In addition, the Transfer Learning-based feature extraction reached higher scores with the “\ac{svm} - Linear” model, in particular with the “Inception-ResNet” architecture, which achieved a weighted \DIFdelbegin \DIFdel{F1 score }\DIFdelend \DIFaddbegin \DIFadd{\fscore{} }\DIFaddend of 0.82 with a deviation of 0.04. With this same model, the Transfer Learning feature extraction methods varied from 0.76 to 0.82, with a deviation range from 0.02 to 0.04. By contrast, all of these architectures \DIFdelbegin \DIFdel{performed poorly in }\DIFdelend \DIFaddbegin \DIFadd{were poorly processed by }\DIFaddend the “\ac{svm} - RBF” model \DIFdelbegin \DIFdel{, but this }\DIFdelend \DIFaddbegin \DIFadd{and }\DIFaddend can be explained by \DIFdelbegin \DIFdel{an excessive number of }\DIFdelend \DIFaddbegin \DIFadd{the high-dimensionality of the extracted }\DIFaddend features that resulted in overfitting despite the cross-validation of \DIFdelbegin \DIFdel{hyper-parameter }\DIFdelend \DIFaddbegin \DIFadd{the regularization term. In this situation, the }\DIFaddend “\DIFdelbegin \DIFdel{C”. The “}\DIFdelend VGG-16” architecture \DIFdelbegin \DIFdel{performed better }\DIFdelend \DIFaddbegin \DIFadd{was more suitable }\DIFaddend with only 512 features than the remaining architectures providing 1,536 or 2,048 features. \DIFdelbegin \DIFdel{For the classification model based on trees, no such difference was seen between }%DIFDELCMD < \ac{cart} %%%
\DIFdel{and }\DIFdelend \DIFaddbegin \DIFadd{In regards to tree-based models, the }\ac{cart} \DIFadd{model weighted \fscore{} varies between 0.69 and 0.71 (deviation between 0.03 and 0.05) and 0.58 to 0.64 (deviation between 0.02 and 0.12), respectively for handcrafted methods and transfer learning methods. On the other hand, the }\ac{gb} \DIFadd{model weighted \fscore{} varies from 0.67 to 0.73 (deviation between 0.04 and 0.07) and 0.78 to 0.81 (deviation between 0.04 and 0.05). The above two sentences can be explained by an overfit in a high dimensional situation for the }\ac{cart} \DIFadd{model and in a low dimensional situation for the }\ac{gb} \DIFadd{model. In opposition, the }\ac{rf} \DIFadd{and }\DIFaddend \ac{ert} \DIFdelbegin \DIFdel{on the handcrafted methods , but a notable difference was found when using the Transfer Learning extraction method. This can be due to the huge number of features related to the “}%DIFDELCMD < \ac{svm} %%%
\DIFdel{- RBF” model. As a partial conclusion, the }\DIFdelend \DIFaddbegin \DIFadd{models were homogeneous along with handcrafted and transfer learning features as they are less prone to overfitting in both low and high dimensional feature spaces. To sum up the aforementioned results, the }\DIFaddend rest of this article retains only the best combination, with “Inception-ResNet” as the feature extraction methods and the Linear \ac{svm} \DIFaddbegin \DIFadd{as the }\DIFaddend classification model.\par
\begin{table}[h]
    \centering
    \DIFdelbeginFL %DIFDELCMD < \begin{tabular}{llcccc}
%DIFDELCMD <     \multicolumn{2}{c}{}                                                        %%%
\DIFdelendFL \DIFaddbeginFL \begin{tabular}{lcccccc}
                                   \DIFaddendFL &\DIFdelbeginFL %DIFDELCMD < \multicolumn{4}{c}{\textbf{Classifier type}}                                                       %%%
\DIFdelendFL \DIFaddbeginFL \multicolumn{6}{c}{\textbf{Classifier type}}                                                                                                         \DIFaddendFL \\ \DIFdelbeginFL %DIFDELCMD < \cline{3-6}
%DIFDELCMD <     \multicolumn{2}{c}{}                                                        %%%
\DIFdelendFL \DIFaddbeginFL \cline{2-7}
                                   \DIFaddendFL &\textbf{\ac{cart}}     &\DIFaddbeginFL \textbf{\ac{rf}}       &\DIFaddendFL \textbf{\ac{ert}}      &\DIFaddbeginFL \textbf{\ac{gb}}       &\DIFaddendFL \textbf{\ac{svm} - Linear} &\textbf{\ac{svm} - RBF}  \\ \hline
    \DIFdelbeginFL %DIFDELCMD < \multirow{2}{*}{\textbf{Spatial}}           &%%%
\DIFdelendFL \textbf{Haralick}              &0.71$\pm$0.05          &0.71$\pm$\DIFaddbeginFL \DIFaddFL{0.04          }&\DIFaddFL{0.71$\pm$}\DIFaddendFL 0.05          &\DIFaddbeginFL \DIFaddFL{0.67$\pm$0.05          }&\DIFaddendFL 0.71$\pm$0.05              &0.70$\pm$0.07            \\ \DIFdelbeginFL %DIFDELCMD < \cline{2-6}
%DIFDELCMD <                                                 &%%%
\DIFdelendFL \DIFaddbeginFL \cline{2-7}
    \DIFaddendFL \textbf{\ac{glh}+\ac{glcm}}    &\textbf{0.71$\pm$0.03} &\DIFaddbeginFL \DIFaddFL{0.71$\pm$0.03          }&\DIFaddendFL 0.72$\pm$0.05          &\DIFaddbeginFL \DIFaddFL{0.67$\pm$0.07          }&\DIFaddendFL 0.70$\pm$0.05              &0.72$\pm$0.04            \\ \hline
    \DIFdelbeginFL %DIFDELCMD < \multirow{2}{*}{\textbf{Frequency}}         &%%%
\DIFdelendFL \textbf{Fourier}               &0.69$\pm$0.05          &0.70$\pm$\DIFaddbeginFL \DIFaddFL{0.03          }&\DIFaddFL{0.70$\pm$}\DIFaddendFL 0.04          &\DIFaddbeginFL \DIFaddFL{0.69$\pm$0.04          }&\DIFaddendFL 0.73$\pm$0.03              &0.69$\pm$0.06            \\ \DIFdelbeginFL %DIFDELCMD < \cline{2-6}
%DIFDELCMD <                                                 &%%%
\DIFdelendFL \DIFaddbeginFL \cline{2-7}
    \DIFaddendFL \textbf{Wavelet}               &0.70$\pm$0.03          &\DIFaddbeginFL \textbf{\DIFaddFL{0.72$\pm$0.04}} &\DIFaddendFL \textbf{0.74$\pm$0.05} &\DIFaddbeginFL \textbf{\DIFaddFL{0.73$\pm$0.05}} &\DIFaddendFL \textbf{0.74$\pm$0.04}     &\textbf{0.72$\pm$0.03}   \\ \hline\DIFdelbeginFL %DIFDELCMD < \multirow{4}{*}{\textbf{Transfer Learning}} &%%%
\DIFdelendFL \DIFaddbeginFL \hline
    \DIFaddendFL \textbf{VGG-16}                &0.58$\pm$0.12          &\DIFaddbeginFL \DIFaddFL{0.73$\pm$0.03          }&\DIFaddendFL 0.76$\pm$0.06          &\DIFaddbeginFL \DIFaddFL{0.78$\pm$0.04          }&\DIFaddendFL 0.76$\pm$0.03              &\textbf{0.64$\pm$0.20}   \\ \DIFdelbeginFL %DIFDELCMD < \cline{2-6}
%DIFDELCMD <                                                 &%%%
\DIFdelendFL \DIFaddbeginFL \cline{2-7}
    \DIFaddendFL \textbf{Inception-V3}          &0.63$\pm$0.04          &\DIFaddbeginFL \DIFaddFL{0.74$\pm$0.06          }&\DIFaddendFL 0.78$\pm$0.04          &0.79$\pm$\DIFaddbeginFL \DIFaddFL{0.05          }&\DIFaddFL{0.79$\pm$}\DIFaddendFL 0.03              &0.44$\pm$0.04            \\ \DIFdelbeginFL %DIFDELCMD < \cline{2-6}
%DIFDELCMD <                                                 &%%%
\DIFdelendFL \DIFaddbeginFL \cline{2-7}
    \DIFaddendFL \textbf{ResNet}                &0.62$\pm$0.08          &\DIFaddbeginFL \DIFaddFL{0.75$\pm$0.05          }&\DIFaddendFL 0.78$\pm$0.02          &\DIFaddbeginFL \textbf{\DIFaddFL{0.81$\pm$0.05}} &\DIFaddendFL 0.79$\pm$0.02              &0.43$\pm$0.05            \\ \DIFdelbeginFL %DIFDELCMD < \cline{2-6}
%DIFDELCMD <                                                 &%%%
\DIFdelendFL \DIFaddbeginFL \cline{2-7}
    \DIFaddendFL \textbf{Inception-ResNet}      &\textbf{0.64$\pm$0.02} &\DIFaddbeginFL \textbf{\DIFaddFL{0.76$\pm$0.03}} &\DIFaddendFL \textbf{0.79$\pm$0.05} &\DIFaddbeginFL \textbf{\DIFaddFL{0.81$\pm$0.05}} &\DIFaddendFL \textbf{0.82$\pm$0.04}     &0.44$\pm$0.06            \\ \hline
    \end{tabular}    
    \caption{List of the results based on combinations of features extraction methods from \Cref{sec:features} and the classification models from \Cref{sec:image_decision} evaluated over a weighted \DIFdelbeginFL \DIFdelFL{F1 score }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{\fscore{} }\DIFaddendFL based on \DIFdelbeginFL \DIFdelFL{Benign }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{benign }\DIFaddendFL and \DIFdelbeginFL \DIFdelFL{Malignant }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{malignant }\DIFaddendFL classifications.}
    \label{tab:image_results}
\end{table}
This paragraph focuses on the methods implemented to reach the patient diagnosis (see \Cref{sec:patient_decision}) and it relates to the initial \ac{rcm} data (including unlabeled images) used previously to evaluate specialists~\cite{Cinotti2018}. All these experimental results are listed in \Cref{tab:patient_results} and discussed below. Firstly, the methods \DIFdelbegin \DIFdel{based on decision }\DIFdelend \DIFaddbegin \DIFadd{performance }\DIFaddend varied between 0.61 and 0.84 in terms of the \DIFdelbegin \DIFdel{F1 score }\DIFdelend \DIFaddbegin \DIFadd{\fscore{} }\DIFaddend for Malignancy. The “At Least One” method achieved \DIFdelbegin \DIFdel{a }\DIFdelend poor performance due to \DIFdelbegin \DIFdel{insufficient results for }\DIFdelend \DIFaddbegin \DIFadd{prediction errors over }\DIFaddend the “\DIFdelbegin \DIFdel{Benign}\DIFdelend \DIFaddbegin \DIFadd{benign}\DIFaddend ” class. This problem can be solved by \DIFaddbegin \DIFadd{the }\DIFaddend use of a dynamic activation threshold for decisions to minimize the risk of false-positives, \DIFdelbegin \DIFdel{but it results in an ethics }\DIFdelend \DIFaddbegin \DIFadd{at the cost of resulting in an ethical }\DIFaddend consideration of this method in the clinical context. Secondly, the methods based on the score are \DIFdelbegin \DIFdel{quite equivalent }\DIFdelend \DIFaddbegin \DIFadd{almost the same }\DIFaddend and varied from 0.76 to 0.83 for the \DIFdelbegin \DIFdel{F1 score }\DIFdelend \DIFaddbegin \DIFadd{\fscore{} }\DIFaddend for Malignancy. By contrast, with these results, the standard deviations remained reasonable, varying from 0.03 to 0.06. Finally, \ac{mil} \DIFdelbegin \DIFdel{were }\DIFdelend \DIFaddbegin \DIFadd{was }\DIFaddend also evaluated, and a substantial difference was noted between the \ac{sil} and the MI-SVM. Indeed, the \ac{sil} assumption yielded similar results with the decision based on the “At Least One” method, due to an insufficient \DIFdelbegin \DIFdel{discrimination capacity }\DIFdelend \DIFaddbegin \DIFadd{ability to discriminate }\DIFaddend on the same “benign” class. By contrast, the MI-SVM yielded a number of good results, with an \DIFdelbegin \DIFdel{F1 score }\DIFdelend \DIFaddbegin \DIFadd{\fscore{} }\DIFaddend of 0.82. Both methods are \DIFdelbegin \DIFdel{quite }\DIFdelend stable, with a deviation that only varied from 0.02 to 0.04. Poor results with the “At Least One” and the \ac{sil} methods can be due to a lack of \DIFdelbegin \DIFdel{discriminate }\DIFdelend \DIFaddbegin \DIFadd{discriminative }\DIFaddend information provided by the “Inception-ResNet” for these methods.\par
\begin{table}[H]
    \centering
    \begin{tabular}{lllll}
                                &                   & \DIFdelbeginFL %DIFDELCMD < \multicolumn{3}{c}{\textbf{Malignancy - F1-Score}}                    %%%
\DIFdelendFL \DIFaddbeginFL \multicolumn{3}{c}{\textbf{Malignancy - \fscore{}}}                    \DIFaddendFL \\ \hline
    \textbf{Category}           & \textbf{Name}     & \textbf{Weighted}     & \textbf{Benign}       & \textbf{Malignant}    \\ \hline
    \multirow{2}{*}{Decision}   & At Least One      & 0.61$\pm$0.06         & 0.32$\pm$0.07         & 0.79$\pm$0.05         \\ \cline{2-5} 
                                & \textbf{Dynamic}  & \textbf{0.84$\pm$0.03}& \textbf{0.78$\pm$0.07}& \textbf{0.87$\pm$0.02}\\ \hline 
    \multirow{2}{*}{Score}      & Mean              & 0.83$\pm$0.03         & 0.78$\pm$0.08         & 0.87$\pm$0.02         \\ \cline{2-5}
                                & Maximum           & 0.76$\pm$0.04         & 0.68$\pm$0.03         & 0.80$\pm$0.05         \\ \hline  
    \multirow{2}{*}{\ac{mil}}   & \ac{sil}          & 0.70$\pm$0.04         & 0.50$\pm$0.10         & 0.83$\pm$0.03         \\ \cline{2-5} 
                                & \textbf{MI-SVM}   & \textbf{0.82$\pm$0.02}& \textbf{0.78$\pm$0.05}& \textbf{0.84$\pm$0.02}\\ \hline 
    \end{tabular}    
    \caption{Results for the patient-level classification for Malignancy (\ac{lm} and \ac{bcc}) according to the different methods from \Cref{sec:patient_decision}. For Malignancy and \ac{lm}, the table provides a weighted average \DIFdelbeginFL \DIFdelFL{F1 score }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{\fscore{} }\DIFaddendFL and individual \DIFdelbeginFL \DIFdelFL{F1 scores }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{\fscore{} }\DIFaddendFL for the \DIFdelbeginFL \DIFdelFL{Benign }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{benign }\DIFaddendFL and the \DIFdelbeginFL \DIFdelFL{Malignant }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{malignant }\DIFaddendFL classes.}
    \label{tab:patient_results}
\end{table}\par
\DIFdelbegin \DIFdel{Consequently to }\DIFdelend \DIFaddbegin \DIFadd{Following }\DIFaddend previous results, this paragraph discusses in detail the results of supervised “Dynamic” decision threshold and \ac{mil} based on “MI-SVM” methods over the Malignancy (meaning \ac{bcc} and \ac{lm}) and \ac{lm} as the cited clinical study does~\cite{Cinotti2018}. \Cref{tab:patient_results_details} provides \DIFdelbegin \DIFdel{F1-Score}\DIFdelend \DIFaddbegin \DIFadd{\fscore{}}\DIFaddend , precision, and recall based on these experiments. As the classification is binary, recall of the positive class refers to the sensitivity and recall of the negative class refers to the Specificity. The “Dynamic” method achieves scores of 0.89$\pm$0.03 sensitivity and 0.75$\pm$0.07 specificity for Malignancy; 0.88$\pm$0.04 sensitivity and 0.75$\pm$0.07 specificity for \ac{lm} pathologies. The “MI-SVM” method achieves scores of 0.80$\pm$0.02 sensitivity and 0.84$\pm$0.05 specificity for Malignancy; 0.78$\pm$0.07 sensitivity and 0.84$\pm$0.07 specificity for \ac{lm} pathologies. The “Dynamic” method provides more emphasis on sensitivity while “MI-SVM” provides a good specificity. These methods are quite \DIFdelbegin \DIFdel{relevant compared }\DIFdelend \DIFaddbegin \DIFadd{comparable }\DIFaddend to the evaluation of the dermatologists, reaching 0.80 of sensitivity and 0.81 of specificity, but less homogeneous compared to them.\par
\par
\begin{table}[H]
    \centering
    \begin{tabular}{lllll||lll}
                                &                   & \multicolumn{3}{c}{\textbf{Malignancy}}                               & \multicolumn{3}{c}{\textbf{\ac{lm}}}                                  \\ \hline
    \textbf{Name}               & \textbf{Label}    & \textbf{\DIFdelbeginFL \DIFdelFL{F1-Score}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{\fscore{}}\DIFaddendFL }     & \textbf{Precision}    & \textbf{Recall}       & \textbf{\DIFdelbeginFL \DIFdelFL{F1-Score}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{\fscore{}}\DIFaddendFL }     & \textbf{Precision}    & \textbf{Recall}       \\ \hline
    \multirow{3}{*}{Dynamic}    & Benign            & 0.78$\pm$0.07         & 0.81$\pm$0.08         & 0.75$\pm$0.07         & 0.79$\pm$0.06         & 0.82$\pm$0.07         & 0.75$\pm$0.07         \\ \cline{2-8}  
                                & Malignant         & 0.87$\pm$0.02         & 0.85$\pm$0.03         & 0.89$\pm$0.03         & 0.86$\pm$0.03         & 0.83$\pm$0.03         & 0.88$\pm$0.04         \\ \cline{2-8} 
                                & Weighted          & 0.84$\pm$0.03         & 0.84$\pm$0.03         & 0.84$\pm$0.03         & 0.83$\pm$0.03         & 0.83$\pm$0.03         & 0.83$\pm$0.03         \\ \hline
    \multirow{3}{*}{MI-SVM}     & Benign            & 0.78$\pm$0.02         & 0.72$\pm$0.08         & 0.84$\pm$0.07         & 0.78$\pm$0.05         & 0.73$\pm$0.07         & 0.84$\pm$0.07         \\ \cline{2-8}
                                & Malignant         & 0.84$\pm$0.02         & 0.89$\pm$0.05         & 0.80$\pm$0.05         & 0.82$\pm$0.03         & 0.87$\pm$0.06         & 0.78$\pm$0.07         \\ \cline{2-8} 
                                & Weighted          & 0.82$\pm$0.02         & 0.82$\pm$0.02         & 0.83$\pm$0.02         & 0.80$\pm$0.03         & 0.80$\pm$0.03         & 0.81$\pm$0.02         \\ \hline 
    \end{tabular}    
    \caption{Detailed results for the patient-level classification for the Decision method based on a Dynamic threshold and \ac{mil} based on the MI-SVM assumption. The table provides the \DIFdelbeginFL \DIFdelFL{F1 scores}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{\fscore{}}\DIFaddendFL , Precision, and Recall for the \DIFdelbeginFL \DIFdelFL{Benign }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{benign }\DIFaddendFL and \DIFdelbeginFL \DIFdelFL{Malignant }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{malignant }\DIFaddendFL classes with these methods.}
    \label{tab:patient_results_details}
\end{table}\par
Finally, \Cref{fig:roc_results} provides \ac{roc} curves for both malignancy and \ac{lm} pathologies on “Dynamic” and “MI-SVM” methods. In the context of Malignancy evaluation, the measured \ac{auc} is 0.89 for “MI-SVM” and 0.88 for “Dynamic”. For \ac{lm} evaluation, the measured \ac{auc} is 0.88 for “MI-SVM” and 0.87 for “Dynamic”\DIFdelbegin \DIFdel{and can be compared with the score of the specialists }\DIFdelend \DIFaddbegin \DIFadd{. In the same context of }\ac{lm} \DIFadd{lesions, the experts obtained an }\ac{auc} \DIFadd{score }\DIFaddend of 0.89\DIFdelbegin \DIFdel{. Based on }%DIFDELCMD < \ac{auc} %%%
\DIFdel{score both of these methods are quite successful.
}%DIFDELCMD < \par
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \DIFadd{, so close to the previous two methods. Apart from this, \mbox{%DIFAUXCMD
\Cref{fig:misclassified} }\hspace{0pt}%DIFAUXCMD
provides some misleading images: the }\ac{rcm} \DIFadd{images in the center belongs to the same patient (image c and d) with similar patterns and homogeneous information while the }\ac{rcm} \DIFadd{images on the outside parts of the figure contain hair, artifacts, tricky patterns or nonhomogeneous information (image a, b, e, and f). Also, the images on the bottom left and the bottom right (image b and e) of the figure are examples of images were experts will use stacks of images to make their decision and where the currently developed methods only use a single image.
}\DIFaddend \begin{figure}[H]
    \centering
    \begin{subfigure}{0.47\linewidth}
        \centering
        \textbf{Malignancy \ac{roc} curves}\par
        \includegraphics[width=\linewidth]{Figures/Result_Malignancy.pdf}
    \end{subfigure}
    \begin{subfigure}{0.47\linewidth}
        \centering
        \textbf{\ac{lm} \ac{roc} curves}\par
        \includegraphics[width=\linewidth]{Figures/Result_LMM.pdf}
    \end{subfigure} 
    \caption{On the left, the \ac{roc} curves for Malignancy with the Dynamic and MI-SVM methods. On the right, the \ac{roc} curves for \ac{lm} with the Dynamic and MI-SVM methods.}
    \label{fig:roc_results}
\end{figure}

\DIFaddbegin \begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Figures/Misclassified.pdf}
    \caption{\DIFaddFL{Examples of }\ac{rcm} \DIFaddFL{images that mislead the classifier with the highest \fscore{} (Inception-ResNet + SVM - Linear). On the left part (image a, b and c), }\ac{rcm} \DIFaddFL{images belonging to the benign label and classified as malignant. On the right part (image d, e and f), }\ac{rcm} \DIFaddFL{images belonging to the malignant label and classified as benign. In the center of the figure (image c and d), two }\ac{rcm} \DIFaddFL{images of the same patient belonging to benign and malignant labels.}}
    \label{fig:misclassified}
\end{figure}
\DIFaddend %=====================================
% DISCUSSION
%=====================================
\section{Conclusions}
\label{sec:conclusions}
This research investigated the classification of malignant tumors and particularly \ac{lm} pathologies at the image-level and at the patient-level. Firstly, at the image-level, an analysis was performed of previous research and proposed methods based on Transfer Learning showing that the Inception-ResNet architecture trained on the ImageNet database is quite relevant for classifying \ac{rcm} images, with a weighted \DIFdelbegin \DIFdel{F1 score }\DIFdelend \DIFaddbegin \DIFadd{\fscore{} }\DIFaddend of 0.82 between benign and malignant labels. Secondly at the patient-level, supervised decision methods were compared to \ac{mil} methods based on feature extraction with the Inception-ResNet architecture. On the first hand, the classification of malignancy over patients achieves a weighted \DIFdelbegin \DIFdel{F1-Score }\DIFdelend \DIFaddbegin \DIFadd{\fscore{} }\DIFaddend of 0.84 with an \ac{auc} score of 0.88 using the supervised method based on a dynamic threshold. It achieves a weighted \DIFdelbegin \DIFdel{F1-Score }\DIFdelend \DIFaddbegin \DIFadd{\fscore{} }\DIFaddend of 0.82 with an \ac{auc} score of 0.89 using the \ac{mil} method based on MI-SVM. On the other hand, the classification of \ac{lm} over patient achieves a weighted \DIFdelbegin \DIFdel{F1-Score }\DIFdelend \DIFaddbegin \DIFadd{\fscore{} }\DIFaddend of 0.83 (Sensitivity 0.88 / Specificity 0.75) with an \ac{auc} score of 0.87 using the supervised method based on a dynamic threshold. It achieves \DIFdelbegin \DIFdel{weighted F1-Score }\DIFdelend \DIFaddbegin \DIFadd{a weighted \fscore{} }\DIFaddend of 0.80 (Sensitivity 0.78 / Specificity 0.84) with an \ac{auc} score of 0.88 using the \ac{mil} method based on MI-SVM. Both techniques are relevant compared to the evaluation of dermatologists, reaching 0.80 of sensitivity and 0.81 of specificity with an \ac{auc} score of 0.89. Furthermore, the supervised method based on the dynamic threshold has more sensitivity that can be relevant in the medical context.\par
Further development of these findings should focus on \DIFaddbegin \DIFadd{the }\DIFaddend enhancement of the image classification through feature extraction enhancement, in particular by investigation of fine-tuning of the \ac{cnn} architecture. In another step, the patient decision should be improved by working on the score meaning through score calibration methods and by rethinking the way the decision is taken as a result of these.\par

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{10pt} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\DIFdelbegin %DIFDELCMD < \funding{This research was funded by the Conseil Regional de Bourgogne Franche-Comte, France and the European Regional Development Fund (ERDF).}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \funding{This research was funded by the Conseil Regional de Bourgogne Franche-Comte of France and the European Regional Development Fund (ERDF).}
\DIFaddend 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\conflictsofinterest{The authors declare no conflict of interest.} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\acknowledgments{We thank Dr. Perrot and Dr. Cinotti for their work on the data and the permission granted to exploit it.\par}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
\DIFdelbegin %DIFDELCMD < \abbreviations{The following abbreviations are used in this manuscript:\\
%DIFDELCMD < 

%DIFDELCMD < \noindent 
%DIFDELCMD < \begin{tabular}{@{}ll}
%DIFDELCMD < \acs{auc}   & \Acl{auc}\\
%DIFDELCMD < \acs{bcc}   & \Acl{bcc}\\
%DIFDELCMD < \acs{cart}  & \Acl{cart}\\
%DIFDELCMD < \acs{cnn}   & \Acl{cnn}\\
%DIFDELCMD < \acs{dej}   & \Acl{dej}\\
%DIFDELCMD < \acs{ert}   & \Acl{ert}\\
%DIFDELCMD < \acs{ggd}   & \Acl{ggd}\\
%DIFDELCMD < \acs{glh}   & \Acl{glh}\\
%DIFDELCMD < \acs{glcm}  & \Acl{glcm}\\
%DIFDELCMD < \acs{lm}    & \Acl{lm}\\
%DIFDELCMD < \acs{mil}   & \Acl{mil}\\
%DIFDELCMD < \acs{rcm}   & \Acl{rcm}\\
%DIFDELCMD < \acs{roc}   & \Acl{roc}\\
%DIFDELCMD < \acs{sil}   & \Acl{sil}\\
%DIFDELCMD < \acs{svm}   & \Acl{svm}\\
%DIFDELCMD < \end{tabular}}
%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \abbreviations{The following abbreviations are used in this manuscript:\\

\noindent 
\begin{tabular}{@{}ll}
\acs{auc}   & \Acl{auc}\\
\acs{bcc}   & \Acl{bcc}\\
\acs{cart}  & \Acl{cart}\\
\acs{cnn}   & \Acl{cnn}\\
\acs{dej}   & \Acl{dej}\\
\acs{ert}   & \Acl{ert}\\
\acs{gb}    & \Acl{gb}\\
\acs{ggd}   & \Acl{ggd}\\
\acs{glh}   & \Acl{glh}\\
\acs{glcm}  & \Acl{glcm}\\
\acs{lm}    & \Acl{lm}\\
\acs{mil}   & \Acl{mil}\\
\acs{rcm}   & \Acl{rcm}\\
\acs{rf}    & \Acl{rf}\\
\acs{roc}   & \Acl{roc}\\
\acs{sil}   & \Acl{sil}\\
\acs{svm}   & \Acl{svm}\\
\end{tabular}}
\DIFaddend 

%=====================================
% References
%=====================================
\reftitle{References}
\externalbibliography{yes}
\bibliography{bibliography}

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
% \sampleavailability{Samples of the compounds ...... are available from the authors.}

%% for journal Sci
%\reviewreports{\\
%Reviewer 1 comments and authors’ response\\
%Reviewer 2 comments and authors’ response\\
%Reviewer 3 comments and authors’ response
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
